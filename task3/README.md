`"Сообщения приходят два раза в день, сообщение записывается в файл формата csv"` <-- в задании не обозначено,
куда поступают файлы и какой у них формат названия, поэтому буду считать, что файлы приходят на сервер, на котором
запускается bash script, в директорию `YYYY-mm-dd/` и имеют произвольное название.

   Если мы примерно знаем, во сколько происходят проверки и приходят файлы, то создаем cron job/Airflow DAG с расписанием на это время,
если нет, то ставим расписание с учетом того, с какой задержкой нам важно получать данные в СУБД, например раз в час.
Job/DAG делает следующее:
    
- ищет файлы в директории `YYYY-mm-dd/`
- загружает данные файлов в таблицу СУБД, партицированную по дню, на текущий день (`TRUNCATE,INSERT`)
(или просто `DELETE,INSERT` без партицирования, если данных мало)
    
ИЛИ
- ищет файлы в директории `YYYY-mm-dd/`
 - удаляет из пула найденных файлов файлы с суффиксом `_processed`
 - загружает данные отфильтрованных файлов в таблицу СУБД инкрементально (`INSERT`)
- добавляет к названиям загруженных файлов суффикс `_processed` (`*.csv` -> `*_processed.csv`)

ИЛИ
   - Аналогично 2-му способу, но данные по загруженным файлам хранятся в отдельной таблице СУБД, и фильтрация
проводится как файлы из директории `YYYY-mm-dd/` минус файлы, которые получаются в результе запроса
`SELECT filename FROM processed_files WHERE file_date = CURRENT_DATE`. Ну и соответственно после загрузки данных в СУБД
файлы добавляются в таблицу `processed_files`. Все действия делаются в одной транзакции, чтобы не допустить несогласованного
состояния БД.

В рамках текущей постановки задачи я бы использовал 1-й способ, так как файлов в день может приходить мало (не больше 4,
как я понял: not_finished,failed,not_finished,failed) и проблем с высоким потреблением ресурсов не будет.
Реализация (вместе с наполнением таблицы `station_errors`) представлена в файле: `upload_station_errors.sh`.
